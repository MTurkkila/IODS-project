# Clustering and classification

In this chapter I look into linear discriminant analysis and k-means clustering using ready made data set from the MASS-library.
please also see [bonus](#bonus), tho not ready yet.

## The Data
Access the required libraries
```{r message=FALSE, warning=FALSE}
library(MASS); library(tidyr); library(ggplot2); library(corrplot)
```

First I load the `Boston` data set and explore it with `str` and `summary`.
```{r}
data("Boston")
str(Boston)
summary(Boston)
```

The `MASS` package contains data sets to accompany book "Modern Applied Statistics with S" by W. N. Venables and B. D. Ripley with several distinct data from different sources. The `Boston` data set contains Housing values in suburbs of Boston. The set includes, for example, variables `crim` that is the per capita crime rate by town and `age` that is proportion of owner-occupied units built prior to 1940. For complete descriptions please see (Boston {MASS})[https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html].

Next, let's plot distributions of the variables. For fun, let's use color of the Faculty of Educational Sciences.
```{r}
gather(Boston) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar(fill="#fcd116", color="#fcd116", alpha=0.9)
```

From the bar charts it is apparent that the data is not normally distributed and, in most cases, strongly skewed.

To asses different relationships of the variables let's plot correlation matrix. I think lower triangular matrix looks better so let's use `type = "lower"` instead `upper` as it was in the datacamp exercises.
```{r}
cor_matrix <-cor (Boston) 
cor_matrix <- cor_matrix %>% round(2)
corrplot(cor_matrix, method="circle", type="lower", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)
```
The per capita crime rate most strongly correlates positively with variables `rad` and `tax` that are index of accessibility to radial highways and full-value property-tax rate per \$10,000 respectively. These two variables are also strongly correlated together. I can not reason for the connection between these variables ans the crime rate.

For the following analyzes we need to standardize the data I use the `scale` function. The function subtracts column mean from each value and then divides it with the standard deviation: $scaled(x) = \frac{x-\bar{x}}{\mu_x}$

We can see this effect with the summary.
```{r}
boston_scaled <- scale(Boston)
summary(boston_scaled)
```
The scaling does not change the distribution of the data but scales all variable values around zero (mean).

The scaled data set is a matrix instead of dataframe, so let's change it back. Replotting distributions, we see that the actual distributions do not change, but the values are 
```{r}
class(boston_scaled)
boston_scaled <- as.data.frame(boston_scaled)
```

Next, I create categorical variable of the crime rate by first dividing the data into four (quantile) bins and then cutting the data to those bins and giving each bin a label. Also, original `crim` variable is removed and the categorical variable added to the scaled data set.
```{r}
bins <- quantile(boston_scaled$crim)
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))
boston_scaled <- dplyr::select(boston_scaled, -crim)
boston_scaled <- data.frame(boston_scaled, crime)
```

Additionally, I divide the data set to train and test sets for future use. To select 80% of the data randomly as the train set, I use `sample` to randomly select `n*0.8` indexes from the scaled data set. Then, I use those index to select the data. Using `[-ind,]` selects the rows not in `[ind,]`.  Lastly, I save the correct crime classes form test set and remove them from that set.
```{r}
n <- nrow(boston_scaled)
ind <- sample(n,  size = n * 0.8)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
correct_classes <- test$crime
test <- dplyr::select(test, -crime)
```

## Linear discriminant analysis

More about LDA in this StatQuest-video  
<iframe width="640" height="385" src="https://www.youtube.com/embed/azXCzI57Yfc" frameborder="0" allowfullscreen></iframe>

For the linear discriminant analysis I use the categorical crime rate created above as the target variable and all other variables as predictors. Data for the fit is the train set previously cut from the `boston_scaled` dataframe. Next, I draw the LDA plot. 
```{r}
palette(c("#fcd116","#00a6ed","#f6511d", "#7fb800")) # Custom color palette
lda.fit <- lda(crime ~., data = train)
classes <- as.numeric(train$crime)
plot(lda.fit, dimen = 2, col = classes, pch = classes)
```

Now, it is possible to predict crime rate categories using the LDA fit. Let's check predictions from the test set agains the correct classes and cross tabulate the results.
```{r}
lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)
```

The predictions are quite good as most values are on the diagonal. Only `med_low` gets several false positives.

Before k-means clustering, I standardize the orginial Boston data set and calculate euclidean and manhattan distance.
```{r}
Boston <- scale(Boston)
Boston <- as.data.frame(Boston)
dist_eu <- dist(Boston)
summary(dist_eu)
dist_man <- dist(Boston, method = "manhattan")
summary(dist_man)
```
The euclidean distances sre clearly shorter as, I think, they should be.

Next I do k-means clustering with three centers and draw correlation plots, with data point colors corresponding with clusters, of a few intresting variables.
```{r}
km <-kmeans(Boston, centers = 3)
pairs(Boston[,c(1,9, 10, 12, 13, 14)], col = km$cluster)
```

To determine optimal number of clusters I calculate and visualize within cluster sum of squares (WCSS) as function of clusters. As `kmeans` assigns initial clusters randomly, I set constant seed used in random number generator.
```{r}
set.seed(123) # the seed for rng
k_max <- 10
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss}) # these lines form datacamp exercise
qplot(x = 1:k_max, y = twcss, geom = 'line') + scale_x_continuous(breaks=c(1:10))
```

The twcss drops most drastically from one to two clusters meaning that two clusters is the optimal number of clusters.

```{r}
km <-kmeans(Boston, centers = 2)
pairs(Boston[,c(1,9, 10, 12, 13, 14)], col = km$cluster)
```

Visually also two clusters looks better than three.

```{r eval=FALSE, include=FALSE}
# need ggally lib. This for nicer plots if time
km <-kmeans(Boston, centers = 2)
ggpairs(data = Boston, aes(col = km$cluster))
```

## BONUS{#bonus}

### Bonus

For this bonus assignment I use LDA fit for three clusters from k-means algorithm. 
```{r}
Boston <- dplyr::select(Boston, -chas) # need remove this variable as the lda.fit doesn't work with it when knitting
km <-kmeans(Boston, centers = 3)
lda.fit <- lda(km$cluster ~., data = Boston)
```

Visualization of clusters and arrows done with code from datacamp. 
```{r}
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(km$cluster)
# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes, )
lda.arrows(lda.fit, myscale = 3, color="black")
```

With three clusters it seem like variables `age`, `zn`, `tax` and `nox` are the most influential separators.

### Super-Bonus
The code below for the (scaled) train data that you used to fit the LDA. The code creates a matrix product, which is a projection of the data points.
```{r}
lda.fit <- lda(crime ~., data = train)
model_predictors <- dplyr::select(train, -crime)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)
```


```{r message=FALSE, warning=FALSE}
library(plotly)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = train$crime)
```

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# this not working. too tired to fix
km <-kmeans(train, centers = 3)
lda.fit <- lda(km$cluster ~., data = train)

model_predictors <- dplyr::select(train, -crime)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)

plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = km$cluster)
```